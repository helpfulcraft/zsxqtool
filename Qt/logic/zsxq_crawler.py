import pprint
import re
import requests
import json
import os
import shutil
import datetime
import urllib.request
from bs4 import BeautifulSoup
from urllib.parse import quote
from urllib.parse import unquote
import base64
import time
import frontmatter
import pprint
from concurrent.futures import ThreadPoolExecutor, as_completed

from PySide6.QtCore import QObject, Signal, QThread

# ===============================================================
# å…¨å±€é…ç½® - è¿™äº›ç°åœ¨ä½œä¸ºåå¤‡ï¼Œä¼˜å…ˆä½¿ç”¨GUIä¼ å…¥çš„å€¼
# ===============================================================
# çŸ¥è¯†æ˜Ÿçƒ API çš„è®¿é—®ä»¤ç‰Œ (å¤‡ç”¨ï¼Œå®é™…å€¼åº”ç”±GUIä¼ å…¥)
ZSXQ_ACCESS_TOKEN_FALLBACK = 'F0107543-661C-4E0E-9D42-CFE769AD1AF9_4E936A0F4E574D35'
# HTTPè¯·æ±‚çš„ç”¨æˆ·ä»£ç†
USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:79.0) Gecko/20100101 Firefox/79.0'
# çŸ¥è¯†æ˜Ÿçƒåœˆå­ID (å¤‡ç”¨ï¼Œå®é™…å€¼åº”ç”±GUIä¼ å…¥)
GROUP_ID_FALLBACK = '88885284844882'

# --- é»˜è®¤æŠ“å–è®¾ç½® (è¿™äº›å€¼ä¼šè¢«GUIè¦†ç›–) ---
DOWLOAD_PICS = True         # æ˜¯å¦ä¸‹è½½å›¾ç‰‡
DOWLOAD_FILES = True        # æ˜¯å¦ä¸‹è½½é™„ä»¶
DOWLOAD_COMMENTS = True     # æ˜¯å¦ä¸‹è½½è¯„è®º
COUNTS_PER_TIME = 20        # æ¯æ¬¡APIè¯·æ±‚è·å–çš„å¸–å­æ•°é‡
SLEEP_FLAG = True           # æ˜¯å¦åœ¨æ¯æ¬¡APIè¯·æ±‚åæš‚åœ
SLEEP_SEC = 1               # æš‚åœç§’æ•°
# ===============================================================

# å…¨å±€å˜é‡
num = 0             # å·²å¤„ç†çš„å¸–å­è®¡æ•°å™¨
g_debug_num = None  # ç”¨äºè°ƒè¯•ï¼Œé™åˆ¶æŠ“å–çš„å¸–å­æ€»æ•°ï¼Œç”±GUIä¼ å…¥


def save_as_markdown(topic_data, output_dir, log_callback=print):
    """
    å°†å•ä¸ªå¸–å­æ•°æ®ä¿å­˜ä¸ºå¸¦æœ‰YAML Front Matterçš„Markdownæ–‡ä»¶ã€‚

    Args:
        topic_data (dict): åŒ…å«å¸–å­æ‰€æœ‰ä¿¡æ¯çš„å­—å…¸ã€‚
        output_dir (str): ä¿å­˜Markdownæ–‡ä»¶çš„ç›®æ ‡ç›®å½•ã€‚
        log_callback (function): ç”¨äºè®°å½•æ—¥å¿—çš„å›è°ƒå‡½æ•°ã€‚
    """
    # åˆ›å»ºä¸€ä¸ªç©ºçš„ frontmatter Post å¯¹è±¡
    post = frontmatter.Post("")

    # è®¾ç½®YAML Front Matterå…ƒæ•°æ®
    post.metadata = {
        'topic_id': topic_data.get('topic_id'),
        'author': topic_data.get('author'),
        'create_time': topic_data.get('create_time'),
        'digested': topic_data.get('digested', False), # æ˜¯å¦ä¸ºç²¾åå¸–
        'image_urls': topic_data.get('image_urls', []), # å¸–å­ä¸­çš„å›¾ç‰‡æœ¬åœ°è·¯å¾„
        'file_paths': topic_data.get('file_paths', []), # å¸–å­ä¸­çš„é™„ä»¶æœ¬åœ°è·¯å¾„
        'likes': topic_data.get('likes', 0), # ç‚¹èµæ•°
        'comments_count': topic_data.get('comments_count', 0), # è¯„è®ºæ•°
    }

    # å‡†å¤‡Markdownå†…å®¹éƒ¨åˆ†
    content_parts = []
    # å¸–å­æ­£æ–‡
    content_parts.append(topic_data.get('text', ''))

    # æ·»åŠ å›¾ç‰‡é™„ä»¶é“¾æ¥
    if topic_data.get('image_urls'):
        content_parts.append("\n\n**å›¾ç‰‡é™„ä»¶:**\n")
        for img_path in topic_data.get('image_urls'):
            img_name = os.path.basename(img_path)
            # ä½¿ç”¨ç›¸å¯¹è·¯å¾„é“¾æ¥åˆ°å›¾ç‰‡
            content_parts.append(f"![image]({img_name})")

    # æ·»åŠ æ–‡ä»¶é™„ä»¶é“¾æ¥
    if topic_data.get('file_paths'):
        content_parts.append("\n\n**æ–‡ä»¶é™„ä»¶:**\n")
        for file_path in topic_data.get('file_paths'):
            file_name = os.path.basename(file_path)
            # ä½¿ç”¨ç›¸å¯¹è·¯å¾„é“¾æ¥åˆ°æ–‡ä»¶
            content_parts.append(f"- [{file_name}]({file_name})")

    # å¦‚æœæ˜¯é—®ç­”å¸–ï¼Œæ·»åŠ å›ç­”éƒ¨åˆ†
    if topic_data.get('answer'):
        content_parts.append(f"\n\n---\n\n**å›ç­” by {topic_data.get('answer_author', '')}:**\n\n{topic_data.get('answer', '')}")

    # æ·»åŠ è¯„è®ºåŒº
    if topic_data.get('comments'):
        content_parts.append("\n\n---\n\n### è¯„è®ºåŒº\n\n")
        for comment in topic_data.get('comments'):
            content_parts.append(comment)

    # å°†æ‰€æœ‰å†…å®¹éƒ¨åˆ†åˆå¹¶ä¸ºæœ€ç»ˆçš„Markdownå†…å®¹
    post.content = "\n".join(content_parts)

    # æ„é€ æœ€ç»ˆæ–‡ä»¶è·¯å¾„
    filepath = os.path.join(output_dir, f"{topic_data.get('topic_id')}.md")
    try:
        # ä½¿ç”¨ frontmatter åº“å°†å…ƒæ•°æ®å’Œå†…å®¹å†™å…¥æ–‡ä»¶
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(frontmatter.dumps(post, allow_unicode=True, sort_keys=False, width=10000))
        log_callback(f"  - [æˆåŠŸ] å·²ä¿å­˜å¸–å­: {filepath}")
    except Exception as e:
        log_callback(f"  - [å¤±è´¥] ä¿å­˜å¸–å­å¤±è´¥: {filepath}, é”™è¯¯: {e}")

def handle_link_to_md(text):
    """
    å¤„ç†çŸ¥è¯†æ˜Ÿçƒç‰¹å®šçš„å¯Œæ–‡æœ¬æ ¼å¼ï¼Œå°†å…¶è½¬æ¢ä¸ºæ ‡å‡†çš„Markdownã€‚
    çŸ¥è¯†æ˜Ÿçƒçš„å¯Œæ–‡æœ¬ä½¿ç”¨ <e> æ ‡ç­¾æ¥è¡¨ç¤ºé“¾æ¥ã€@æŸäººã€è¯é¢˜ç­‰ã€‚

    Args:
        text (str): ä»APIè·å–çš„åŸå§‹HTMLæ ¼å¼æ–‡æœ¬ã€‚

    Returns:
        str: è½¬æ¢åçš„Markdownæ ¼å¼æ–‡æœ¬ã€‚
    """
    if not text:
        return ""
    # æ›¿æ¢æ¢è¡Œç¬¦
    text = text.replace('<br/>', '\n').replace('<br>', '\n')
    # ä½¿ç”¨BeautifulSoupè§£æHTML
    soup = BeautifulSoup(text, "html.parser")
    # å¤„ç†åŠ ç²—æ–‡æœ¬
    for bold in soup.select('e[type="text_bold"]'):
        title = unquote(bold.attrs.get('title', ''))
        bold.replace_with(f"# {title}\n\n")
    # å¤„ç†@ç”¨æˆ·
    for mention in soup.select('e[type="mention"]'):
        mention.replace_with(f"@{mention.attrs.get('title', '')}")
    # å¤„ç†#è¯é¢˜#
    for tag in soup.select('e[type="hashtag"]'):
        tag.replace_with(f"#{unquote(tag.attrs.get('title', ''))}")
    # å¤„ç†ç½‘é¡µé“¾æ¥
    for link in soup.select('e[type="web"]'):
        title = unquote(link.attrs.get('title', ''))
        href = unquote(link.attrs.get('href', ''))
        link.replace_with(f"[{title}]({href})")
    # å¤„ç†æ ‡å‡†çš„<a>æ ‡ç­¾
    for a in soup.find_all('a'):
        href = a.get('href', '')
        link_text = a.get_text(strip=True)
        if href and link_text:
            if link_text == href:
                 a.replace_with(href) # å¦‚æœé“¾æ¥æ–‡æœ¬å’ŒURLç›¸åŒï¼Œåˆ™åªä¿ç•™URL
            else:
                 a.replace_with(f"[{link_text}]({href})") # è½¬æ¢ä¸ºMarkdowné“¾æ¥
        elif href:
            a.replace_with(href) # å¦‚æœåªæœ‰hrefï¼Œåˆ™åªä¿ç•™URL
        else:
            a.unwrap() # å¦‚æœæ²¡æœ‰hrefï¼Œåˆ™ç§»é™¤aæ ‡ç­¾ï¼Œä¿ç•™å†…éƒ¨æ–‡æœ¬
    return soup.get_text()

def get_data(url, output_dir, log_callback, token, is_single_post=False):
    """
    æ ¸å¿ƒå‡½æ•°ï¼šé€šè¿‡APIè·å–å¸–å­æ•°æ®ï¼Œå¤„ç†å¹¶ä¿å­˜ã€‚æ”¯æŒåˆ†é¡µé€’å½’è°ƒç”¨ã€‚

    Args:
        url (str): è¦è¯·æ±‚çš„API URLã€‚
        output_dir (str): ä¿å­˜æ–‡ä»¶çš„ç›®å½•ã€‚
        log_callback (function): æ—¥å¿—å›è°ƒå‡½æ•°ã€‚
        token (str): çŸ¥è¯†æ˜Ÿçƒçš„access_tokenã€‚
        is_single_post (bool): æ ‡è®°å½“å‰æ˜¯å¦ä¸ºæŠ“å–å•ä¸ªå¸–å­æ¨¡å¼ã€‚
    """
    global num, g_debug_num # ä½¿ç”¨å…¨å±€å˜é‡æ¥è®¡æ•°å’Œæ§åˆ¶æ€»æ•°
    headers = {
        'Cookie': 'zsxq_access_token=' + token,
        'User-Agent': USER_AGENT
    }
    log_callback("æ­£åœ¨è¯·æ±‚URL: " + url)

    # è¯·æ±‚é‡è¯•é€»è¾‘
    retry_count = 0
    while True:
        try:
            rsp = requests.get(url, headers=headers, timeout=30)
            response_json = rsp.json()
            if response_json.get('succeeded'):
                break # è¯·æ±‚æˆåŠŸï¼Œè·³å‡ºé‡è¯•å¾ªç¯
            if response_json.get('code') == 1059: # çŸ¥è¯†æ˜ŸçƒAPIçš„ä¸€ä¸ªå¸¸è§å†…éƒ¨é”™è¯¯ç 
                retry_count += 1
                log_callback(f"APIè¿”å›å†…éƒ¨é”™è¯¯(1059)ï¼Œå°†åœ¨ {SLEEP_SEC} ç§’åè¿›è¡Œç¬¬ {retry_count} æ¬¡é‡è¯•...")
                if SLEEP_FLAG:
                    time.sleep(SLEEP_SEC)
                continue # ç»§ç»­ä¸‹ä¸€æ¬¡é‡è¯•
            else:
                log_callback(f"é”™è¯¯: APIé€»è¾‘å¤±è´¥ï¼Œæ— æ³•é‡è¯•. å“åº”: {response_json}")
                return # æ— æ³•å¤„ç†çš„é”™è¯¯ï¼Œå‡½æ•°è¿”å›

        except requests.exceptions.RequestException as e:
            retry_count += 1
            log_callback(f"é”™è¯¯: ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}ï¼Œå°†åœ¨ {SLEEP_SEC} ç§’åè¿›è¡Œç¬¬ {retry_count} æ¬¡é‡è¯•...")
            if SLEEP_FLAG:
                time.sleep(SLEEP_SEC)
            continue
        except json.JSONDecodeError:
            retry_count += 1
            log_callback(f"é”™è¯¯: è§£æJSONå¤±è´¥ï¼Œå°†åœ¨ {SLEEP_SEC} ç§’åè¿›è¡Œç¬¬ {retry_count} æ¬¡é‡è¯•...")
            if SLEEP_FLAG:
                time.sleep(SLEEP_SEC)
            continue

    try:
        resp_data = response_json.get('resp_data', {})
        
        topics = []
        # æ ¹æ®æ˜¯å¦æ˜¯å•å¸–æ¨¡å¼ï¼Œä»ä¸åŒçš„JSONç»“æ„ä¸­æå–å¸–å­åˆ—è¡¨
        if is_single_post:
            if 'topic' in resp_data:
                topics = [resp_data['topic']]
        else:
            raw_items = resp_data.get('topics', [])
            # APIåœ¨ä¸åŒç«¯ç‚¹è¿”å›çš„topicsåˆ—è¡¨ç»“æ„å¯èƒ½ä¸ä¸€è‡´ï¼š
            # - /groups/.../topics ç›´æ¥è¿”å› topic å¯¹è±¡åˆ—è¡¨ [topic, topic, ...]
            # - /search/topics å¯èƒ½è¿”å›åŒ…è£…è¿‡çš„å¯¹è±¡åˆ—è¡¨ [{'topic': {...}}, ...]
            # æ­¤å¤„è¿›è¡Œå…¼å®¹æ€§å¤„ç†
            if raw_items and 'topic' in raw_items[0]:
                # å…¼å®¹åŒ…è£…è¿‡çš„åˆ—è¡¨ï¼ˆä¾‹å¦‚æœç´¢ç»“æœï¼‰
                topics = [item.get('topic') for item in raw_items if item.get('topic')]
            else:
                # å…¼å®¹ç›´æ¥çš„topicå¯¹è±¡åˆ—è¡¨
                topics = raw_items
        
        if not topics:
            log_callback("æœªæ‰¾åˆ°æ›´å¤šå¸–å­ã€‚")
            return

        # éå†è·å–åˆ°çš„æ¯ä¸€ä¸ªå¸–å­
        for topic in topics:
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°äº†è°ƒè¯•è®¾ç½®çš„æŠ“å–æ•°é‡ä¸Šé™
            if g_debug_num is not None and num >= g_debug_num:
                break
            
            topic_id = topic.get('topic_id')
            if not topic_id:
                log_callback("  - [è­¦å‘Š] å¸–å­ç¼ºå°‘topic_idï¼Œå·²è·³è¿‡ã€‚")
                continue
                
            # æ£€æŸ¥å¸–å­æ˜¯å¦å·²ç»ä¸‹è½½è¿‡ï¼Œå¦‚æœå­˜åœ¨åˆ™è·³è¿‡
            output_filepath = os.path.join(output_dir, f"{topic_id}.md")
            if os.path.exists(output_filepath):
                log_callback(f"  - [è·³è¿‡] å¸–å­ {topic_id} å·²å­˜åœ¨ã€‚")
                continue

            num += 1
            log_callback(f"æ­£åœ¨å¤„ç†ç¬¬ {num} ä¸ªå¸–å­ (ID: {topic_id})...")
            
            # å¸–å­å†…å®¹å¯èƒ½åœ¨ä¸åŒçš„å­—æ®µä¸­ï¼ˆtalk, question, task, solutionï¼‰
            content = topic.get('question', topic.get('talk', topic.get('task', topic.get('solution'))))
            if not content:
                log_callback("  - [è­¦å‘Š] å¸–å­å†…å®¹ä¸ºç©ºï¼Œå·²è·³è¿‡ã€‚")
                continue

            # å¤„ç†å¸–å­æ­£æ–‡ä¸­çš„å¯Œæ–‡æœ¬
            parsed_text = handle_link_to_md(content.get('text', ''))
            # å¤„ç†æ–‡ç« é“¾æ¥ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if content.get('article'):
                article_title = content.get('article', {}).get('title', 'é˜…è¯»åŸæ–‡')
                article_url = content.get('article', {}).get('article_url', '')
                if article_url:
                    parsed_text += f"\n\n---\nğŸ”— [{article_title}]({article_url})"

            # å‡†å¤‡è¦ä¿å­˜çš„å¸–å­æ•°æ®å­—å…¸
            topic_data = {
                'topic_id': str(topic_id),
                'author': content.get('owner', {}).get('name', 'åŒ¿åç”¨æˆ·') if not content.get('anonymous') else 'åŒ¿åç”¨æˆ·',
                'create_time': (topic.get('create_time')[:23]).replace('T', ' '), # æ ¼å¼åŒ–æ—¶é—´
                'digested': topic.get('digested', False),
                'likes': topic.get('likes_count', 0),
                'comments_count': topic.get('comments_count', 0),
                'text': parsed_text,
                'image_urls': [],
                'file_paths': [],
            }

            # ä¸‹è½½å›¾ç‰‡
            if DOWLOAD_PICS and content.get('images'):
                # ä¸ºæ¯ä¸ªå¸–å­åˆ›å»ºä¸€ä¸ªç‹¬ç«‹çš„å­ç›®å½•å­˜æ”¾å›¾ç‰‡å’Œé™„ä»¶
                image_dir = os.path.join(output_dir, str(topic_id))
                os.makedirs(image_dir, exist_ok=True)
                # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘ä¸‹è½½å›¾ç‰‡
                with ThreadPoolExecutor(max_workers=5) as executor:
                    future_to_url = {executor.submit(download_image, img.get('large', {}).get('url'), os.path.join(image_dir, os.path.basename(img.get('large', {}).get('url').split('?')[0])), log_callback): img.get('large', {}).get('url') for img in content.get('images') if img.get('large', {}).get('url')}
                    for future in as_completed(future_to_url):
                        img_url = future_to_url[future]
                        try:
                            img_local_path = future.result()
                            if img_local_path:
                                # å°†ä¸‹è½½æˆåŠŸçš„å›¾ç‰‡æœ¬åœ°è·¯å¾„æ·»åŠ åˆ°æ•°æ®ä¸­
                                topic_data['image_urls'].append(img_local_path)
                        except Exception as exc:
                            log_callback(f'    - [å›¾ç‰‡] ä¸‹è½½ç”Ÿæˆå¼‚å¸¸: {img_url}, é”™è¯¯: {exc}')
            
            # ä¸‹è½½é™„ä»¶
            if DOWLOAD_FILES and content.get('files'):
                file_dir = os.path.join(output_dir, str(topic_id))
                os.makedirs(file_dir, exist_ok=True)
                # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘ä¸‹è½½é™„ä»¶
                with ThreadPoolExecutor(max_workers=3) as executor:
                    future_to_file = {executor.submit(download_file, f.get('file_id'), os.path.join(file_dir, f.get('name')), token, log_callback): f.get('name') for f in content.get('files') if f.get('file_id') and f.get('name')}
                    for future in as_completed(future_to_file):
                        file_name = future_to_file[future]
                        try:
                            file_local_path = future.result()
                            if file_local_path:
                                # å°†ä¸‹è½½æˆåŠŸçš„é™„ä»¶æœ¬åœ°è·¯å¾„æ·»åŠ åˆ°æ•°æ®ä¸­
                                topic_data['file_paths'].append(file_local_path)
                        except Exception as exc:
                            log_callback(f'    - [é™„ä»¶] ä¸‹è½½ç”Ÿæˆå¼‚å¸¸: {file_name}, é”™è¯¯: {exc}')

            # å¦‚æœæ˜¯é—®ç­”å¸–ï¼Œæå–å›ç­”å†…å®¹
            if topic.get('question'):
                topic_data['answer_author'] = topic.get('answer', {}).get('owner',{}).get('name', '')
                topic_data['answer'] = handle_link_to_md(topic.get('answer', {}).get('text', ""))

            # å¤„ç†è¯„è®º
            topic_data['comments'] = []
            if DOWLOAD_COMMENTS and topic.get('show_comments'):
                for comment in topic.get('show_comments'):
                    author_name = comment.get('owner', {}).get('name', 'æœªçŸ¥ç”¨æˆ·')
                    comment_text = handle_link_to_md(comment.get('text', ''))
                    repliee = comment.get('repliee') # è¢«å›å¤çš„äºº
                    if repliee:
                        repliee_name = repliee.get('name', 'æœªçŸ¥ç”¨æˆ·')
                        # æ ¼å¼åŒ–ä¸º "A å›å¤ B: ..."
                        topic_data['comments'].append(f"> **{author_name}** å›å¤ **{repliee_name}**: {comment_text}\n")
                    else:
                        # æ ¼å¼åŒ–ä¸º "A: ..."
                        topic_data['comments'].append(f"> **{author_name}**: {comment_text}\n")
            
            # æ‰€æœ‰æ•°æ®å¤„ç†å®Œæ¯•ï¼Œä¿å­˜ä¸ºMarkdownæ–‡ä»¶
            save_as_markdown(topic_data, output_dir, log_callback)

    except Exception as e:
        import traceback
        log_callback(f"å¤„ç†å“åº”æ•°æ®æ—¶å‘ç”Ÿä¸¥é‡å¼‚å¸¸: {e}")
        log_callback(traceback.format_exc())
        return

    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°è°ƒè¯•æ•°é‡é™åˆ¶
    if g_debug_num is not None and num >= g_debug_num:
        log_callback(f"å·²è¾¾åˆ°è®¾å®šçš„æŠ“å–æ•°é‡ä¸Šé™ ({g_debug_num})ï¼Œä»»åŠ¡å®Œæˆã€‚")
        return

    # å¦‚æœæ˜¯æŠ“å–å•å¸–æ¨¡å¼ï¼Œåˆ°æ­¤ç»“æŸ
    if is_single_post:
        log_callback("å•å¸–æŠ“å–å®Œæˆã€‚")
        return
        
    # --- åˆ†é¡µé€»è¾‘ ---
    next_page_data = response_json.get('resp_data', {})
    # æ£€æŸ¥è¿”å›çš„æ•°æ®ä¸­æ˜¯å¦è¿˜æœ‰ä¸‹ä¸€é¡µçš„å¸–å­
    if 'topics' in next_page_data and next_page_data.get('topics'):
        topics_in_page = next_page_data['topics']
        last_topic_item = None
        # å…¼å®¹ä¸åŒAPIè¿”å›çš„ç»“æ„ï¼Œä»¥æ­£ç¡®è·å–ç”¨äºåˆ†é¡µçš„æœ€åä¸€ä¸ªtopic
        if 'topic' in topics_in_page[0]:
            # å…¼å®¹æœç´¢æ¥å£è¿”å›çš„åŒ…è£…ç»“æ„
            for item in reversed(topics_in_page):
                if item.get('topic'):
                    last_topic_item = item.get('topic')
                    break
        else:
            # å…¼å®¹æ™®é€šå¸–å­åˆ—è¡¨æ¥å£ï¼Œç›´æ¥å–æœ€åä¸€ä¸ª
            last_topic_item = topics_in_page[-1]
        
        if last_topic_item:
            create_time = last_topic_item.get('create_time')
            if not create_time:
                 log_callback("è­¦å‘Š: æ— æ³•ä»æœ€åä¸€ä¸ªå¸–å­è·å–create_timeï¼Œåˆ†é¡µä¸­æ–­ã€‚")
                 return

            # çŸ¥è¯†æ˜Ÿçƒçš„åˆ†é¡µæ˜¯é€šè¿‡ä¸Šä¸€é¡µæœ€åä¸€ä¸ªå¸–å­çš„åˆ›å»ºæ—¶é—´æ¥å®ç°çš„
            end_time = quote(create_time)
            # å¯¹æ—¶é—´å­—ç¬¦ä¸²è¿›è¡Œç‰¹æ®Šå¤„ç†ä»¥ç¬¦åˆAPIè¦æ±‚
            if len(end_time) == 33:
                end_time = end_time[:24] + '0' + end_time[24:]
            
            # æ„å»ºä¸‹ä¸€é¡µçš„URL
            base_next_url = url.split('&end_time=')[0]
            next_url = f"{base_next_url}&end_time={end_time}"

            # æš‚åœä¸€æ®µæ—¶é—´ï¼Œé˜²æ­¢è¯·æ±‚è¿‡äºé¢‘ç¹
            if SLEEP_FLAG:
                time.sleep(SLEEP_SEC)
            # é€’å½’è°ƒç”¨è‡ªèº«æ¥è·å–ä¸‹ä¸€é¡µçš„æ•°æ®
            get_data(next_url, output_dir, log_callback, token, is_single_post=False)
        else:
            log_callback("æ‰€æœ‰å¸–å­å¤„ç†å®Œæ¯• (æœ€åä¸€ä¸ªåˆ†é¡µæ‰¹æ¬¡æ— æœ‰æ•ˆtopic)ã€‚")
    else:
        log_callback("æ‰€æœ‰å¸–å­å¤„ç†å®Œæ¯• (APIæœªè¿”å›æ›´å¤štopics)ã€‚")

def download_image(url, local_path, log_callback):
    """
    ä¸‹è½½å•ä¸ªå›¾ç‰‡ã€‚

    Args:
        url (str): å›¾ç‰‡çš„URLã€‚
        local_path (str): æœ¬åœ°ä¿å­˜è·¯å¾„ã€‚
        log_callback (function): æ—¥å¿—å›è°ƒå‡½æ•°ã€‚

    Returns:
        str or None: æˆåŠŸåˆ™è¿”å›æœ¬åœ°è·¯å¾„ï¼Œå¤±è´¥åˆ™è¿”å›Noneã€‚
    """
    try:
        r = requests.get(url, stream=True, timeout=20)
        r.raise_for_status() # å¦‚æœè¯·æ±‚å¤±è´¥ï¼ˆé2xxçŠ¶æ€ç ï¼‰ï¼Œåˆ™æŠ›å‡ºå¼‚å¸¸
        with open(local_path, 'wb') as f:
            for chunk in r.iter_content(chunk_size=8192):
                f.write(chunk)
        log_callback(f"    - [å›¾ç‰‡] å·²ä¸‹è½½: {local_path}")
        return local_path
    except Exception as e:
        log_callback(f"    - [å›¾ç‰‡] ä¸‹è½½å¤±è´¥: {url}, é”™è¯¯: {e}")
        return None

def download_file(file_id, local_path, token, log_callback):
    """
    ä¸‹è½½å•ä¸ªé™„ä»¶ã€‚é™„ä»¶ä¸‹è½½éœ€è¦å…ˆé€šè¿‡ä¸€ä¸ªAPIè·å–çœŸå®çš„ä¸‹è½½é“¾æ¥ã€‚

    Args:
        file_id (str): é™„ä»¶çš„IDã€‚
        local_path (str): æœ¬åœ°ä¿å­˜è·¯å¾„ã€‚
        token (str): çŸ¥è¯†æ˜Ÿçƒaccess_tokenã€‚
        log_callback (function): æ—¥å¿—å›è°ƒå‡½æ•°ã€‚

    Returns:
        str or None: æˆåŠŸåˆ™è¿”å›æœ¬åœ°è·¯å¾„ï¼Œå¤±è´¥åˆ™è¿”å›Noneã€‚
    """
    # è·å–ä¸‹è½½é“¾æ¥çš„API
    download_url_api = f"https://api.zsxq.com/v2/files/{file_id}/download_url"
    headers = {'Cookie': f'zsxq_access_token={token}', 'User-Agent': USER_AGENT}
    try:
        # ç¬¬ä¸€æ­¥ï¼šè¯·æ±‚APIè·å–çœŸå®ä¸‹è½½é“¾æ¥
        r = requests.get(download_url_api, headers=headers, timeout=20)
        r.raise_for_status()
        resp_json = r.json()
        if not resp_json.get('succeeded'):
            log_callback(f"    - [é™„ä»¶] è·å–ä¸‹è½½é“¾æ¥APIå¤±è´¥: {resp_json}")
            return None
        
        real_download_url = resp_json.get('resp_data', {}).get('download_url')
        if not real_download_url:
            log_callback(f"    - [é™„ä»¶] æœªæ‰¾åˆ°ä¸‹è½½é“¾æ¥: {file_id}")
            return None

        # ç¬¬äºŒæ­¥ï¼šä½¿ç”¨è·å–åˆ°çš„é“¾æ¥ä¸‹è½½æ–‡ä»¶
        r_file = requests.get(real_download_url, stream=True, timeout=60)
        r_file.raise_for_status()
        with open(local_path, 'wb') as f:
            for chunk in r_file.iter_content(chunk_size=8192):
                f.write(chunk)
        log_callback(f"    - [é™„ä»¶] å·²ä¸‹è½½: {local_path}")
        return local_path
    except Exception as e:
        log_callback(f"    - [é™„ä»¶] ä¸‹è½½æ–‡ä»¶å¤±è´¥: ID={file_id}, é”™è¯¯={e}")
        return None

def run_crawler(crawl_mode, group_id, token, search_keyword="", post_id="", debug_num=None, log_callback=print):
    """
    çˆ¬è™«ä¸»å…¥å£å‡½æ•°ï¼Œç”±GUIè°ƒç”¨ã€‚

    Args:
        crawl_mode (str): æŠ“å–æ¨¡å¼ ('all', 'digests', 'search', 'single_post')ã€‚
        group_id (str): æ˜ŸçƒIDã€‚
        token (str): ç”¨æˆ·access_tokenã€‚
        search_keyword (str, optional): æœç´¢æ¨¡å¼ä¸‹çš„å…³é”®è¯ã€‚
        post_id (str, optional): å•å¸–æ¨¡å¼ä¸‹çš„å¸–å­IDã€‚
        debug_num (int, optional): é™åˆ¶æŠ“å–çš„å¸–å­æ•°é‡ã€‚
        log_callback (function, optional): æ—¥å¿—å›è°ƒå‡½æ•°ã€‚
    """
    global num, g_debug_num
    num = 0 # é‡ç½®å…¨å±€è®¡æ•°å™¨
    
    # ä¼˜å…ˆä½¿ç”¨GUIä¼ å…¥çš„å‚æ•°ï¼Œå¦‚æœä¸ºç©ºåˆ™ä½¿ç”¨æ–‡ä»¶é¡¶éƒ¨çš„åå¤‡å€¼
    group_id = group_id or GROUP_ID_FALLBACK
    token = token or ZSXQ_ACCESS_TOKEN_FALLBACK

    # è®¾ç½®è°ƒè¯•ç”¨çš„æŠ“å–æ•°é‡
    if debug_num:
        try:
            g_debug_num = int(debug_num)
        except (ValueError, TypeError):
            log_callback(f"è­¦å‘Šï¼šæ— æ•ˆçš„æŠ“å–æ•°é‡ '{debug_num}'ã€‚å°†æŠ“å–æ‰€æœ‰å¸–å­ã€‚")
            g_debug_num = None
    else:
        g_debug_num = None
    
    # --- è®¾ç½®è¾“å‡ºç›®å½• ---
    script_dir = os.path.dirname(os.path.abspath(__file__))
    qt_dir = os.path.dirname(script_dir)
    output_dir_base = os.path.join(qt_dir, 'output')
    output_dir = ''

    # æ ¹æ®ä¸åŒçš„æŠ“å–æ¨¡å¼ï¼Œç¡®å®šä¸åŒçš„è¾“å‡ºå­ç›®å½•åç§°
    if crawl_mode == 'digests':
        output_dir = os.path.join(output_dir_base, 'raw_md_digests')
    elif crawl_mode == 'search':
        # æ¸…ç†å…³é”®è¯ä¸­çš„éæ³•å­—ç¬¦ï¼Œç”¨ä½œç›®å½•å
        sanitized_keyword = re.sub(r'[\s\\/*?:"<>|]+', "_", search_keyword).strip('_')
        output_dir = os.path.join(output_dir_base, f'raw_md_search_{sanitized_keyword}')
    elif crawl_mode == 'single_post':
        output_dir = os.path.join(output_dir_base, f'raw_md_post_{post_id}')
    else: # 'all'
        output_dir = os.path.join(output_dir_base, 'raw_md')
    
    # æ‰“å°ä»»åŠ¡ä¿¡æ¯
    log_callback("-" * 50)
    log_callback(f"æŠ“å–æ¨¡å¼: '{crawl_mode}'")
    if crawl_mode == 'search': log_callback(f"æœç´¢å…³é”®è¯: '{search_keyword}'")
    if crawl_mode == 'single_post': log_callback(f"å¸–å­ID: '{post_id}'")
    if debug_num is not None: log_callback(f"æŠ“å–æ•°é‡: {debug_num}")
    log_callback(f"æ–‡ä»¶å°†ä¿å­˜åˆ°: {output_dir}")
    log_callback("-" * 50)

    # åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    os.makedirs(output_dir, exist_ok=True)

    # --- æ„é€ èµ·å§‹URL ---
    base_url = f"https://api.zsxq.com/v2/groups/{group_id}"
    start_url = ''

    if crawl_mode == 'digests':
        # è·å–ç²¾åå¸–çš„URL
        start_url = f"{base_url}/topics?scope=digests&count={COUNTS_PER_TIME}"
    elif crawl_mode == 'search':
        # è·å–æœç´¢ç»“æœçš„URL
        keyword_encoded = quote(search_keyword)
        start_url = f"https://api.zsxq.com/v2/search/topics?keyword={keyword_encoded}&group_id={group_id}&count={COUNTS_PER_TIME}&sort=create_time"
    elif crawl_mode == 'single_post':
        # è·å–å•ä¸ªå¸–å­çš„URL
        start_url = f"https://api.zsxq.com/v2/topics/{post_id}"
    else: # 'all'
        # è·å–å…¨éƒ¨å¸–å­çš„URL
        start_url = f"{base_url}/topics?count={COUNTS_PER_TIME}"

    log_callback("å¼€å§‹æŠ“å–å¸–å­...")
    log_callback(f"èµ·å§‹URL: {start_url}")
    
    # è°ƒç”¨æ ¸å¿ƒå‡½æ•°å¼€å§‹æŠ“å–
    get_data(start_url, output_dir, log_callback, token, is_single_post=(crawl_mode == 'single_post'))

    log_callback("æŠ“å–å®Œæˆï¼")
